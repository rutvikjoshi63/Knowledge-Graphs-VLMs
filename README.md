# Knowledge-Graphs-VLMs
## 1. Introduction

In emergency care, triage is a critical process that involves gathering initial patient information, measuring vital signs, understanding symptoms, medical history, and assigning an urgency level to prioritize treatment [1]. Conventional triage relies on human expertise and judgment, which can be subjective and error-prone. Recent studies have shown growing interest in leveraging artificial intelligence (AI) to assist triage, using patient data (vital signs, symptoms, history) to accurately classify patients into risk categories. AI-driven triage systems have even demonstrated higher accuracy than traditional methods in some cases. Robotic triage assistants could further enhance this process by performing contact-free vital monitoring (e.g., via camera sensors) and reducing healthcare workers’ exposure to hazards.

Vision-Language Models (VLMs) offer a promising foundation for triage robotics because they can interpret visual inputs and produce descriptive or analytical text. Models such as CLIP and DINOv2 learn rich visual representations from large-scale data, achieving all-purpose feature extraction across diverse images. For example, DINOv2 is a state-of-the-art vision transformer that produces robust features without supervision [2], which could be applied to patient imagery (e.g. facial temperature cues, respiratory motions). However, domain-specific understanding remains a challenge: a general VLM-LLM may lack medical knowledge needed to interpret abnormal vital signs or subtle symptoms. Large language models can hallucinate or produce plausible-sounding but incorrect assessments when confronted with unfamiliar scenarios due to the inherent limitations of LLMs [3]. Ensuring accurate and up-to-date medical reasoning is essential for patient safety, explainability and to build trust in VLM generation.

To overcome these limitations, we propose to augment VLM-based triage systems with external medical knowledge. In particular, we integrate a Knowledge Graph (KG) capturing relationships between patients, symptoms, and observations (vital signs) into the VLM’s inference process. Knowledge graphs are structured networks of medical facts that can provide context (e.g., which combinations of symptoms indicate high risk, normal ranges for vitals given patient history). By coupling the VLM with a KG, we enable the system to cross-reference visual observations with established medical knowledge, reducing the reliance on the model’s parametric memory alone.

Our approach adopts a Retrieval-Augmented Generation (RAG) paradigm to inject relevant knowledge into the model on-the-fly. RAG has recently gained traction as a solution for hallucination and knowledge update issues in LLMs [3]. It works by retrieving pertinent external information (traditionally text documents) based on the input, and conditioning the model’s output on this retrieved content. Here, we extend RAG to operate over a structured knowledge graph. By retrieving subgraphs of medical facts rather than free text, we preserve the structural relationships in the knowledge (which vanilla RAG might neglect). This structured retrieval is crucial in medicine, where the context (e.g., temporal trends in vitals, symptom co-occurrence) affects interpretation.

In summary, our contributions are as follows:

* **Knowledge Graph-Integrated VLM Architecture:** We propose a novel triage system architecture that tightly integrates a vision-language model with a medical knowledge graph via retrieval-augmented generation. To our knowledge, this is the first application of KG-augmented VLMs for autonomous patient vital assessment in robotics.
* **Adaptive Knowledge Retrieval Mechanism:** Building on the K-RAGRec framework, we develop an adaptive retrieval policy to fetch the most relevant portion of the knowledge graph for a given patient’s data. This mechanism indexes and ranks subgraphs of the KG in real-time, providing the VLM with focused, contextual medical knowledge while filtering out noise.
* **Graph Neural Network Encoding of Medical Subgraphs:** We employ a GNN encoder to transform the retrieved subgraph (containing patient symptoms and observations) into a rich embedding that can interface with the VLM’s language modality. This allows the model to incorporate structured medical facts (as a “graph prompt”) into its reasoning process, improving interpretability and accuracy.
* **Triage Knowledge Graph Design:** We introduce a specialized knowledge graph schema for the triage domain, defining node types (Patient, Symptom, Observation) and their relationships (e.g., patient-observation, observation-symptom edges), including the temporal dimension of patient vitals. This schema enables capturing each patient’s history and the general medical knowledge in a unified graph structure.

The rest of the paper is organized as follows: Section Related Work reviews prior efforts in VLMs, knowledge graphs, and retrieval augmentation in healthcare. Section Background provides necessary context on vision-language models, knowledge graphs, and RAG. In Section Proposed Method, we detail our framework, including the system architecture and KG schema. Sections for Experiments, Results, and Discussion are placeholders for future work. Finally, Conclusion and Future Work outlines the implications of our approach and avenues for further development.

![Report](VLM_final_report.pdf)

This research, conducted under the supervision of Prof. Galeotti at the RI, CMU, investigates methods to enhance VLMs, such as DINOv2, through the integration of Knowledge Graphs and Retrieval-Augmented Generation (RAG). The objective is to improve the automated assessment of patient vital signs as part of the DARPA-funded project.
